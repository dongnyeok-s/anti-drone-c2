#!/bin/bash
# Full Evaluation + ë…¼ë¬¸ìš© Figures ìë™ ìƒì„± ìŠ¤í¬ë¦½íŠ¸

set -e

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$PROJECT_ROOT"

echo "============================================================"
echo "  Full Evaluation + ë…¼ë¬¸ìš© Figures ìë™ ìƒì„±"
echo "============================================================"
echo ""

# 1. runtime_params.json ì¤€ë¹„
echo "[1/5] runtime_params.json ì¤€ë¹„ ì¤‘..."
if [ ! -f "simulator/config/runtime_params.json" ]; then
    if [ -f "analysis/results/auto_tune_best_config.json" ]; then
        python3 -c "
import json
with open('analysis/results/auto_tune_best_config.json', 'r') as f:
    config = json.load(f)
with open('simulator/config/runtime_params.json', 'w') as f:
    json.dump(config['best_params'], f, indent=2)
"
        echo "  âœ“ runtime_params.json ìƒì„± ì™„ë£Œ"
    else
        echo "  âš  best_config.jsonì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤."
    fi
else
    echo "  âœ“ runtime_params.json ì´ë¯¸ ì¡´ì¬"
fi

# 2. Full Evaluation ì‹¤í–‰
echo ""
echo "[2/5] Full Evaluation ì‹¤í–‰ ì¤‘..."
echo "  (ì˜ˆìƒ ì†Œìš” ì‹œê°„: 1-2ì‹œê°„)"
cd analysis
python3 scripts/run_evaluation.py --profile full

# 3. ë¦¬í¬íŠ¸ ìƒì„±
echo ""
echo "[3/5] ë…¼ë¬¸ìš© ë¦¬í¬íŠ¸ ë° Figure ìƒì„± ì¤‘..."
python3 scripts/generate_report.py --full

# 4. ì™„ë£Œ ë©”ì‹œì§€
echo ""
echo "============================================================"
echo "  ëª¨ë“  ì‘ì—… ì™„ë£Œ!"
echo "============================================================"
echo ""
echo "ğŸ“ ìƒì„±ëœ íŒŒì¼:"
echo "  - ë¦¬í¬íŠ¸: analysis/results/full_evaluation_summary.md"
echo "  - Metrics CSV: analysis/results/metrics_table.csv"
echo "  - ë¹„êµ CSV: analysis/results/fusion_vs_baseline_table.csv"
echo "  - Figures: analysis/figures/latest/"
echo ""

