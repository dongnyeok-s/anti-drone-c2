"""
í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (í†µí•© ë²„ì „)

Full Evaluation ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¦¬í¬íŠ¸ì™€ ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python generate_report.py [--full] [--no-plots]
"""

import json
import sys
import csv
from pathlib import Path
from datetime import datetime
from typing import Dict, List
import subprocess
import argparse

PROJECT_ROOT = Path(__file__).parent.parent.parent
SIMULATOR_DIR = PROJECT_ROOT / 'simulator'
ANALYSIS_DIR = PROJECT_ROOT / 'analysis'
RESULTS_DIR = ANALYSIS_DIR / 'results'
LOGS_DIR = SIMULATOR_DIR / 'logs' / 'eval_full'
PLOTS_DIR = ANALYSIS_DIR / 'plots'
FIGURES_DIR = ANALYSIS_DIR / 'figures'

def load_metrics_for_mode(mode: str) -> Dict[str, Dict]:
    """ëª¨ë“œë³„ metrics ë¡œë“œ"""
    # ë¨¼ì € ê¸°ì¡´ metrics.json íŒŒì¼ í™•ì¸
    metrics_json = RESULTS_DIR / mode / 'metrics.json'
    if metrics_json.exists():
        try:
            with open(metrics_json, 'r') as f:
                metrics = json.load(f)
                if metrics:  # ë¹ˆ dictê°€ ì•„ë‹Œ ê²½ìš°
                    return metrics
        except Exception as e:
            print(f"  âš  {mode} metrics.json ì½ê¸° ì˜¤ë¥˜: {e}")
    
    # ì—†ìœ¼ë©´ ë¡œê·¸ ë””ë ‰í† ë¦¬ì—ì„œ ì§ì ‘ ë¶„ì„
    mode_logs_dir = LOGS_DIR / mode
    if not mode_logs_dir.exists():
        # eval_comparison ë””ë ‰í† ë¦¬ë„ í™•ì¸
        mode_logs_dir = PROJECT_ROOT / 'simulator' / 'logs' / 'eval_comparison' / mode
        if not mode_logs_dir.exists():
            return {}
    
    # eval_classification_report.py ì‹¤í–‰
    try:
        result = subprocess.run(
            [sys.executable, 'scripts/eval_classification_report.py',
             '--logs-dir', str(mode_logs_dir),
             '--output-dir', str(RESULTS_DIR / mode)],
            cwd=str(ANALYSIS_DIR),
            capture_output=True,
            text=True,
            timeout=600
        )
        
        if result.returncode == 0:
            metrics_json = RESULTS_DIR / mode / 'metrics.json'
            if metrics_json.exists():
                with open(metrics_json, 'r') as f:
                    return json.load(f)
    except Exception as e:
        print(f"  âš  {mode} metrics ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    return {}

def generate_summary_markdown(all_metrics: Dict[str, Dict[str, Dict]]) -> str:
    """Full evaluation summary ë§ˆí¬ë‹¤ìš´ ìƒì„±"""
    content = []
    content.append("# Full Evaluation Summary\n")
    content.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    content.append("\n## ì‹¤í—˜ ê°œìš”\n")
    content.append("- **í‰ê°€ ëª¨ë“œ**: baseline, fusion_default, fusion_tuned\n")
    content.append("- **í”„ë¡œíŒŒì¼**: Full (ì •ì‹ í‰ê°€)\n")
    content.append("- **ì‹œë‚˜ë¦¬ì˜¤**: all_hostile, mixed_civil, civil_only\n")
    content.append("- **Runs**: 20 runs per experiment\n")
    
    content.append("\n## ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ ë¹„êµ\n")
    
    scenarios = ['all_hostile', 'mixed_civil', 'civil_only']
    modes = ['baseline', 'fusion_default', 'fusion_tuned']
    
    for scenario in scenarios:
        content.append(f"\n### {scenario.upper()}\n")
        content.append("| ëª¨ë“œ | Accuracy | HOSTILE F1 | HOSTILE Precision | HOSTILE Recall | CIVIL FP Rate |\n")
        content.append("|------|----------|------------|-------------------|----------------|---------------|\n")
        
        for mode in modes:
            if mode in all_metrics:
                found = False
                for key, metrics in all_metrics[mode].items():
                    if scenario in key.lower():
                        acc = metrics.get('accuracy', 0) * 100
                        f1 = metrics.get('HOSTILE_f1', 0) * 100
                        prec = metrics.get('HOSTILE_precision', 0) * 100
                        rec = metrics.get('HOSTILE_recall', 0) * 100
                        fp_rate = metrics.get('civil_fp_rate', 0) * 100
                        
                        content.append(f"| {mode} | {acc:.2f}% | {f1:.2f}% | {prec:.2f}% | {rec:.2f}% | {fp_rate:.2f}% |\n")
                        found = True
                        break
                if not found:
                    content.append(f"| {mode} | - | - | - | - | - |\n")
            else:
                content.append(f"| {mode} | - | - | - | - | - |\n")
    
    return ''.join(content)

def generate_metrics_csv(all_metrics: Dict[str, Dict[str, Dict]]) -> Path:
    """Metrics CSV ìƒì„±"""
    csv_path = RESULTS_DIR / 'metrics_table.csv'
    
    scenarios = ['all_hostile', 'mixed_civil', 'civil_only']
    modes = ['baseline', 'fusion_default', 'fusion_tuned']
    
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['Mode', 'Scenario', 'Accuracy', 'HOSTILE_F1', 'HOSTILE_Precision', 
                         'HOSTILE_Recall', 'CIVIL_FP_Rate', 'CIVIL_TP', 'CIVIL_FP', 'CIVIL_FN'])
        
        for mode in modes:
            for scenario in scenarios:
                if mode in all_metrics:
                    for key, metrics in all_metrics[mode].items():
                        if scenario in key.lower():
                            writer.writerow([
                                mode,
                                scenario,
                                f"{metrics.get('accuracy', 0):.4f}",
                                f"{metrics.get('HOSTILE_f1', 0):.4f}",
                                f"{metrics.get('HOSTILE_precision', 0):.4f}",
                                f"{metrics.get('HOSTILE_recall', 0):.4f}",
                                f"{metrics.get('civil_fp_rate', 0):.4f}",
                                metrics.get('CIVIL_TP', 0),
                                metrics.get('CIVIL_FP', 0),
                                metrics.get('CIVIL_FN', 0),
                            ])
                            break
    
    return csv_path

def generate_fusion_vs_baseline_csv(all_metrics: Dict[str, Dict[str, Dict]]) -> Path:
    """Fusion vs Baseline ë¹„êµ CSV ìƒì„±"""
    csv_path = RESULTS_DIR / 'fusion_vs_baseline_table.csv'
    
    scenarios = ['all_hostile', 'mixed_civil', 'civil_only']
    
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['Scenario', 'Metric', 'Baseline', 'Fusion_Default', 'Fusion_Tuned', 
                         'Improvement_Default', 'Improvement_Tuned'])
        
        for scenario in scenarios:
            baseline_metrics = {}
            fusion_default_metrics = {}
            fusion_tuned_metrics = {}
            
            # ê° ëª¨ë“œì—ì„œ metrics ì°¾ê¸°
            for mode in ['baseline', 'fusion_default', 'fusion_tuned']:
                if mode in all_metrics:
                    for key, metrics in all_metrics[mode].items():
                        key_lower = key.lower()
                        if scenario in key_lower:
                            if mode == 'baseline':
                                baseline_metrics = metrics
                            elif mode == 'fusion_default':
                                fusion_default_metrics = metrics
                            elif mode == 'fusion_tuned':
                                fusion_tuned_metrics = metrics
                            break
            
            # ì£¼ìš” ì§€í‘œ ë¹„êµ
            metrics_to_compare = [
                ('Accuracy', 'accuracy'),
                ('HOSTILE_F1', 'HOSTILE_f1'),
                ('HOSTILE_Precision', 'HOSTILE_precision'),
                ('HOSTILE_Recall', 'HOSTILE_recall'),
                ('CIVIL_FP_Rate', 'civil_fp_rate'),
            ]
            
            for metric_name, metric_key in metrics_to_compare:
                baseline_val = baseline_metrics.get(metric_key, 0) if baseline_metrics else 0
                default_val = fusion_default_metrics.get(metric_key, 0) if fusion_default_metrics else 0
                tuned_val = fusion_tuned_metrics.get(metric_key, 0) if fusion_tuned_metrics else 0
                
                improvement_default = (default_val - baseline_val) * 100 if baseline_val > 0 else 0
                improvement_tuned = (tuned_val - baseline_val) * 100 if baseline_val > 0 else 0
                
                writer.writerow([
                    scenario,
                    metric_name,
                    f"{baseline_val:.4f}",
                    f"{default_val:.4f}",
                    f"{tuned_val:.4f}",
                    f"{improvement_default:+.2f}%p",
                    f"{improvement_tuned:+.2f}%p",
                ])
    
    return csv_path

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±')
    parser.add_argument('--full', action='store_true',
                       help='ì „ì²´ ë¦¬í¬íŠ¸ ìƒì„± (í”Œë¡¯ í¬í•¨)')
    parser.add_argument('--no-plots', action='store_true',
                       help='í”Œë¡¯ ìƒì„± ìŠ¤í‚µ')
    
    args = parser.parse_args()
    
    print("="*60)
    print("  í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±")
    print("="*60)
    
    # 1. Full eval ê²°ê³¼ ì½ê¸°
    print("\n[1/4] Full Evaluation ê²°ê³¼ ë¡œë“œ ì¤‘...")
    all_metrics = {}
    modes = ['baseline', 'fusion_default', 'fusion_tuned']
    
    for mode in modes:
        print(f"  {mode} ë¶„ì„ ì¤‘...")
        metrics = load_metrics_for_mode(mode)
        if metrics:
            all_metrics[mode] = metrics
            print(f"    âœ“ {len(metrics)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ ì™„ë£Œ")
        else:
            print(f"    âš  ë°ì´í„° ì—†ìŒ")
    
    if not all_metrics:
        print("\nâœ— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("  Full Evaluationì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”:")
        print("    python analysis/scripts/run_evaluation.py")
        return
    
    # 2. í”Œë¡¯ ìƒì„± (ì˜µì…˜)
    if args.full and not args.no_plots:
        print("\n[2/4] ë…¼ë¬¸ìš© Figure ìƒì„± ì¤‘...")
        try:
            result = subprocess.run(
                [sys.executable, 'plots/generate_all_figures.py'],
                cwd=str(ANALYSIS_DIR),
                capture_output=True,
                text=True,
                timeout=1800
            )
            
            if result.returncode == 0:
                print("  âœ“ ëª¨ë“  Figure ìƒì„± ì™„ë£Œ")
            else:
                print(f"  âš  Figure ìƒì„± ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ")
                print(result.stderr[:500])
        except Exception as e:
            print(f"  âš  Figure ìƒì„± ì˜¤ë¥˜: {e}")
    else:
        print("\n[2/4] í”Œë¡¯ ìƒì„± ìŠ¤í‚µ")
    
    # 3. í…Œì´ë¸” ìƒì„±
    print("\n[3/4] í‰ê°€ í…Œì´ë¸” ìƒì„± ì¤‘...")
    
    # Summary Markdown
    summary_md = generate_summary_markdown(all_metrics)
    summary_path = RESULTS_DIR / 'full_evaluation_summary.md'
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(summary_md)
    print(f"  âœ“ {summary_path}")
    
    # Metrics CSV
    metrics_csv = generate_metrics_csv(all_metrics)
    print(f"  âœ“ {metrics_csv}")
    
    # Fusion vs Baseline CSV
    comparison_csv = generate_fusion_vs_baseline_csv(all_metrics)
    print(f"  âœ“ {comparison_csv}")
    
    # 4. ìµœì¢… ì¶œë ¥
    print("\n[4/4] ì™„ë£Œ!")
    print("="*60)
    
    figures_dir = FIGURES_DIR / 'latest'
    
    print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
    print(f"  - ë¦¬í¬íŠ¸: {summary_path}")
    print(f"  - Metrics CSV: {metrics_csv}")
    print(f"  - ë¹„êµ CSV: {comparison_csv}")
    if args.full and not args.no_plots:
        print(f"  - Figures: {figures_dir}")
    
    print("\nğŸ“Š í•µì‹¬ ì§€í‘œ ìš”ì•½:")
    if 'fusion_default' in all_metrics and 'fusion_tuned' in all_metrics:
        for scenario in ['all_hostile', 'mixed_civil', 'civil_only']:
            print(f"\n  {scenario.upper()}:")
            for mode in ['fusion_default', 'fusion_tuned']:
                if mode in all_metrics:
                    for key, metrics in all_metrics[mode].items():
                        if scenario in key.lower():
                            print(f"    {mode}: F1={metrics.get('HOSTILE_f1', 0):.3f}, "
                                  f"FP={metrics.get('civil_fp_rate', 0):.3f}")
                            break
    
    print("\n" + "="*60)
    print("  ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("="*60)

if __name__ == '__main__':
    main()

