"""
ìë™ íŒŒë¼ë¯¸í„° íŠœë‹ ìŠ¤í¬ë¦½íŠ¸

ëœë¤ ì„œì¹˜ ê¸°ë°˜ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì‹œë„í•˜ê³ ,
ê° ì¡°í•©ì— ëŒ€í•´ í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì—¬
ìµœì ì˜ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python auto_tune.py --trials 50
    python auto_tune.py --trials 50 --seed 12345
"""

import json
import sys
import argparse
import subprocess
import shutil
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict
import random

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
PROJECT_ROOT = Path(__file__).parent.parent
SIMULATOR_DIR = PROJECT_ROOT / 'simulator'
ANALYSIS_DIR = PROJECT_ROOT / 'analysis'
CONFIG_DIR = SIMULATOR_DIR / 'config'

# ê²€ìƒ‰ ê³µê°„ ì„í¬íŠ¸
sys.path.insert(0, str(ANALYSIS_DIR))
from auto_tuning_config import DEFAULT_PARAM_SPACE, sample_params

# ============================================
# ì„¤ì •
# ============================================

RUNTIME_PARAMS_FILE = CONFIG_DIR / 'runtime_params.json'
EVAL_LOGS_DIR = SIMULATOR_DIR / 'logs' / 'eval'
RESULTS_DIR = ANALYSIS_DIR / 'results'
TUNING_HISTORY_FILE = RESULTS_DIR / 'auto_tune_history.json'
BEST_CONFIG_FILE = RESULTS_DIR / 'auto_tune_best_config.json'

# ============================================
# Objective í•¨ìˆ˜
# ============================================

def compute_objective_score(metrics_dict: Dict[Tuple[str, str], Dict]) -> float:
    """
    ì„±ëŠ¥ ì§€í‘œë¥¼ í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ scoreë¡œ ê³„ì‚°
    
    Objective ìˆ˜ì‹:
    score = F1_hostile_all_hostile
          + F1_hostile_mixed_civil
          - 2.0 * civil_fp_rate_mixed_civil
          + 0.3 * accuracy_all_hostile
    
    ëª©í‘œ:
    - HOSTILE F1 ì ìˆ˜ë¥¼ ë†’ì´ê¸° (all_hostile, mixed_civil ì‹œë‚˜ë¦¬ì˜¤)
    - CIVIL False Positiveë¥¼ ê°•í•˜ê²Œ íŒ¨ë„í‹° (ë¯¼ê°„ê¸°ë¥¼ ì ìœ¼ë¡œ ì˜¤ë¶„ë¥˜í•˜ëŠ” ê²ƒ ë°©ì§€)
    - ì „ì²´ ì •í™•ë„ë„ ì–´ëŠ ì •ë„ ë°˜ì˜
    
    Args:
        metrics_dict: {
            ('all_hostile', 'FUSION'): { accuracy, HOSTILE_f1, civil_fp_rate, ... },
            ('mixed_civil', 'FUSION'): { accuracy, HOSTILE_f1, civil_fp_rate, ... },
            ...
        }
    
    Returns:
        objective score (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ, ìŒìˆ˜ ê°€ëŠ¥)
    """
    score = 0.0
    
    # ê¸°ì¤€ ì‹œë‚˜ë¦¬ì˜¤: all_hostile, mixed_civil (FUSION ëª¨ë“œ)
    key_all_hostile = ('all_hostile', 'FUSION')
    key_mixed_civil = ('mixed_civil', 'FUSION')
    
    # 1. HOSTILE F1 ì ìˆ˜ (all_hostile)
    if key_all_hostile in metrics_dict:
        metrics = metrics_dict[key_all_hostile]
        f1_hostile = metrics.get('HOSTILE_f1', 0.0)
        score += f1_hostile * 1.0  # ê°€ì¤‘ì¹˜ 1.0
        
        # ì „ì²´ ì •í™•ë„ ë³´ë„ˆìŠ¤
        accuracy = metrics.get('accuracy', 0.0)
        score += accuracy * 0.3  # ê°€ì¤‘ì¹˜ 0.3
    
    # 2. HOSTILE F1 ì ìˆ˜ (mixed_civil)
    if key_mixed_civil in metrics_dict:
        metrics = metrics_dict[key_mixed_civil]
        f1_hostile = metrics.get('HOSTILE_f1', 0.0)
        score += f1_hostile * 1.0  # ê°€ì¤‘ì¹˜ 1.0
        
        # 3. CIVIL False Positive íŒ¨ë„í‹° (mixed_civilì—ì„œ CIVILì„ HOSTILEë¡œ ì˜¤ë¶„ë¥˜)
        # civil_fp_rate: true_label == CIVILì¸ë° pred_label == HOSTILEì¸ ë¹„ìœ¨
        civil_fp_rate = metrics.get('civil_fp_rate', 0.0)
        score -= civil_fp_rate * 2.0  # ê°•í•œ íŒ¨ë„í‹° (ê°€ì¤‘ì¹˜ -2.0)
    
    return score


# ============================================
# íŒŒë¼ë¯¸í„° ì£¼ì…
# ============================================

def save_runtime_params(params: Dict[str, Any]) -> None:
    """íŒŒë¼ë¯¸í„°ë¥¼ runtime_params.json íŒŒì¼ë¡œ ì €ì¥"""
    CONFIG_DIR.mkdir(exist_ok=True)
    
    with open(RUNTIME_PARAMS_FILE, 'w', encoding='utf-8') as f:
        json.dump(params, f, indent=2)
    
    print(f"  íŒŒë¼ë¯¸í„° ì €ì¥: {RUNTIME_PARAMS_FILE}")


def clear_runtime_params() -> None:
    """runtime_params.json íŒŒì¼ ì‚­ì œ (ê¸°ë³¸ê°’ ì‚¬ìš©)"""
    if RUNTIME_PARAMS_FILE.exists():
        RUNTIME_PARAMS_FILE.unlink()


# ============================================
# í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# ============================================

def run_evaluation(profile: str = 'fast') -> bool:
    """eval runner ì‹¤í–‰"""
    print(f"  [1/2] í‰ê°€ ì‹¤í—˜ ì‹¤í–‰ ì¤‘... (í”„ë¡œíŒŒì¼: {profile})")
    try:
        # í”„ë¡œíŒŒì¼ì— ë”°ë¼ ë‹¤ë¥¸ ëª…ë ¹ ì‹¤í–‰
        if profile == 'fast':
            cmd = ['npm', 'run', 'eval:fast']
        else:
            cmd = ['npm', 'run', 'eval:full']
        
        result = subprocess.run(
            cmd,
            cwd=str(SIMULATOR_DIR),
            capture_output=True,
            text=True,
            timeout=7200 if profile == 'full' else 3600  # fullì€ ë” ê¸´ íƒ€ì„ì•„ì›ƒ
        )
        
        if result.returncode != 0:
            print(f"  âœ— í‰ê°€ ì‹¤í—˜ ì‹¤íŒ¨: {result.stderr}")
            return False
        
        print("  âœ“ í‰ê°€ ì‹¤í—˜ ì™„ë£Œ")
        return True
    except subprocess.TimeoutExpired:
        print("  âœ— í‰ê°€ ì‹¤í—˜ íƒ€ì„ì•„ì›ƒ")
        return False
    except Exception as e:
        print(f"  âœ— í‰ê°€ ì‹¤í—˜ ì˜¤ë¥˜: {e}")
        return False


def run_analysis() -> Dict[Tuple[str, str], Dict]:
    """ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ì½ê¸°"""
    print("  [2/2] ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
    try:
        result = subprocess.run(
            ['python', 'scripts/eval_classification_report.py',
             '--logs-dir', '../simulator/logs/eval',
             '--output-dir', 'results'],
            cwd=str(ANALYSIS_DIR),
            capture_output=True,
            text=True,
            timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        if result.returncode != 0:
            print(f"  âœ— ë¶„ì„ ì‹¤íŒ¨: {result.stderr}")
            return {}
        
        print("  âœ“ ë¶„ì„ ì™„ë£Œ")
        
        # metricsë¥¼ ì½ì–´ì„œ ë°˜í™˜
        # ì‹¤ì œë¡œëŠ” eval_classification_report.pyê°€ JSONì„ ìƒì„±í•˜ë„ë¡ ìˆ˜ì • í•„ìš”
        # ì¼ë‹¨ classification_summary.mdë¥¼ íŒŒì‹±í•˜ê±°ë‚˜, ë³„ë„ JSON ìƒì„± í•„ìš”
        
        return parse_metrics_from_report()
    except subprocess.TimeoutExpired:
        print("  âœ— ë¶„ì„ íƒ€ì„ì•„ì›ƒ")
        return {}
    except Exception as e:
        print(f"  âœ— ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {}


def parse_metrics_from_report() -> Dict[Tuple[str, str], Dict]:
    """
    metrics.jsonì—ì„œ metrics íŒŒì‹±
    
    Returns:
        {
            ('all_hostile', 'FUSION'): { accuracy, HOSTILE_f1, ... },
            ('mixed_civil', 'FUSION'): { accuracy, HOSTILE_f1, ... },
            ...
        }
    """
    # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ê²½ë¡œ í™•ì¸
    possible_paths = [
        RESULTS_DIR / 'metrics.json',
        ANALYSIS_DIR / 'results' / 'metrics.json',
        Path('results') / 'metrics.json',  # ìƒëŒ€ ê²½ë¡œ
    ]
    
    metrics_json_path = None
    for path in possible_paths:
        if path.exists():
            metrics_json_path = path
            break
    
    if not metrics_json_path:
        print(f"  âœ— Metrics JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"    í™•ì¸í•œ ê²½ë¡œ:")
        for path in possible_paths:
            print(f"      - {path} (ì¡´ì¬: {path.exists()})")
        return {}
    
    try:
        with open(metrics_json_path, 'r', encoding='utf-8') as f:
            metrics_dict = json.load(f)
        
        # í‚¤ë¥¼ íŠœí”Œë¡œ ë³€í™˜
        result = {}
        for key_str, metrics in metrics_dict.items():
            # í‚¤ í˜•ì‹: "all_hostile_FUSION", "mixed_civil_FUSION", "civil_only_FUSION"
            # ë˜ëŠ”: "all_hostile_BASELINE" ë“±
            
            # ì–¸ë”ìŠ¤ì½”ì–´ë¡œ splití•˜ë˜, ë§ˆì§€ë§‰ ë¶€ë¶„ì´ ëª¨ë“œ(BASELINE/FUSION)ì¸ì§€ í™•ì¸
            parts = key_str.split('_')
            
            if len(parts) >= 2:
                # ë§ˆì§€ë§‰ ë¶€ë¶„ì´ ëª¨ë“œì¸ì§€ í™•ì¸
                last_part = parts[-1]
                if last_part in ['BASELINE', 'FUSION']:
                    # ì‹œë‚˜ë¦¬ì˜¤ëŠ” ë‚˜ë¨¸ì§€ ë¶€ë¶„ì„ í•©ì¹¨
                    scenario = '_'.join(parts[:-1])
                    mode = last_part
                    result[(scenario, mode)] = metrics
                else:
                    # ëª¨ë“œê°€ ì—†ëŠ” ê²½ìš°, ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ì—ì„œ ì¶”ì¶œ
                    if 'all_hostile' in key_str:
                        result[('all_hostile', 'FUSION')] = metrics
                    elif 'mixed_civil' in key_str:
                        result[('mixed_civil', 'FUSION')] = metrics
                    elif 'civil_only' in key_str:
                        result[('civil_only', 'FUSION')] = metrics
        
        return result
    except Exception as e:
        print(f"  âœ— Metrics JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return {}


# ============================================
# ë©”ì¸ íŠœë‹ ë£¨í”„
# ============================================

def run_auto_tune(trials: int, seed: Optional[int] = None, profile: str = 'fast') -> None:
    """ìë™ íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰"""
    
    # ì‹œë“œ ì„¤ì •
    if seed is None:
        seed = random.randint(1, 1000000)
    rng = random.Random(seed)
    
    print('=' * 60)
    print('  ìë™ íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘')
    print('=' * 60)
    print(f'ì‹œë“œ: {seed}')
    print(f'ì‹œí–‰ íšŸìˆ˜: {trials}')
    print(f'ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {RESULTS_DIR}')
    print()
    
    # ê²°ê³¼ ì €ì¥ìš©
    history: List[Dict[str, Any]] = []
    best_score = float('-inf')  # ìŒìˆ˜ë„ í—ˆìš©í•˜ë¯€ë¡œ -infë¡œ ì´ˆê¸°í™”
    best_params: Optional[Dict[str, Any]] = None
    best_metrics: Optional[Dict[str, Dict]] = None
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    RESULTS_DIR.mkdir(exist_ok=True)
    
    for trial in range(trials):
        print(f'\n[Trial {trial + 1}/{trials}]')
        print('-' * 60)
        
        # 1. íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
        params = sample_params(DEFAULT_PARAM_SPACE, rng)
        print(f"  íŒŒë¼ë¯¸í„°:")
        print(f"    - threat_engage_threshold: {params['threat_engage_threshold']}")
        print(f"    - civil_conf_threshold: {params['civil_conf_threshold']}")
        print(f"    - pn_nav_constant: {params['pn_nav_constant']}")
        print(f"    - sensor_radar_weight: {params['sensor_radar_weight']}")
        
        # 2. íŒŒë¼ë¯¸í„° ì €ì¥
        save_runtime_params(params)
        
        # 3. í‰ê°€ ì‹¤í—˜ ì‹¤í–‰
        if not run_evaluation(profile):
            print(f"  âœ— Trial {trial + 1} ì‹¤íŒ¨ (í‰ê°€ ì‹¤í—˜ ì˜¤ë¥˜)")
            clear_runtime_params()
            continue
        
        # 4. ë¶„ì„ ì‹¤í–‰
        metrics = run_analysis()
        
        if not metrics:
            print(f"  âœ— Trial {trial + 1} ì‹¤íŒ¨ (ë¶„ì„ ì˜¤ë¥˜)")
            clear_runtime_params()
            continue
        
        # 5. Objective score ê³„ì‚°
        score = compute_objective_score(metrics)
        print(f"  Objective Score: {score:.4f}")
        
        # 6. Best ì—…ë°ì´íŠ¸
        if score > best_score:
            best_score = score
            best_params = params.copy()
            best_metrics = metrics.copy()
            print(f"  ğŸ¯ NEW BEST! (ì´ì „: {best_score:.4f})")
        
        # 7. íˆìŠ¤í† ë¦¬ ê¸°ë¡
        history.append({
            'trial': trial + 1,
            'params': params,
            'score': score,
            'metrics': metrics,
        })
        
        # 8. ì¤‘ê°„ ì €ì¥ (ë§¤ 10íšŒë§ˆë‹¤)
        if (trial + 1) % 10 == 0:
            save_history(history, best_params, best_score, best_metrics)
            print(f"\n  ì¤‘ê°„ ì €ì¥ ì™„ë£Œ (Trial {trial + 1})")
        
        # 9. íŒŒë¼ë¯¸í„° íŒŒì¼ ì •ë¦¬
        clear_runtime_params()
    
    # ìµœì¢… ì €ì¥
    save_history(history, best_params, best_score, best_metrics)
    
    # ê²°ê³¼ ì¶œë ¥
    print('\n' + '=' * 60)
    print('  íŠœë‹ ì™„ë£Œ!')
    print('=' * 60)
    print(f'\nìµœê³  ì ìˆ˜: {best_score:.4f}')
    print(f'\nìµœì  íŒŒë¼ë¯¸í„°:')
    if best_params:
        for key, value in best_params.items():
            if isinstance(value, dict):
                print(f"  {key}:")
                for k, v in value.items():
                    print(f"    {k}: {v:.4f}")
            else:
                print(f"  {key}: {value}")
    
    print(f'\nê²°ê³¼ íŒŒì¼:')
    print(f"  - {TUNING_HISTORY_FILE}")
    print(f"  - {BEST_CONFIG_FILE}")


def save_history(
    history: List[Dict],
    best_params: Optional[Dict],
    best_score: float,
    best_metrics: Optional[Dict]
) -> None:
    """íˆìŠ¤í† ë¦¬ ë° ìµœì  ì„¤ì • ì €ì¥"""
    
    # íˆìŠ¤í† ë¦¬ ì €ì¥ (íŠœí”Œ í‚¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜)
    history_serializable = []
    for entry in history:
        entry_copy = entry.copy()
        if 'metrics' in entry_copy and isinstance(entry_copy['metrics'], dict):
            # íŠœí”Œ í‚¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            metrics_str = {}
            for key, value in entry_copy['metrics'].items():
                if isinstance(key, tuple):
                    key_str = f"{key[0]}_{key[1]}"
                else:
                    key_str = str(key)
                metrics_str[key_str] = value
            entry_copy['metrics'] = metrics_str
        history_serializable.append(entry_copy)
    
    with open(TUNING_HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump({
            'total_trials': len(history),
            'best_score': best_score,
            'history': history_serializable,
        }, f, indent=2)
    
    # ìµœì  ì„¤ì • ì €ì¥ (íŠœí”Œ í‚¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜)
    if best_params:
        best_metrics_serializable = None
        if best_metrics:
            best_metrics_serializable = {}
            for key, value in best_metrics.items():
                if isinstance(key, tuple):
                    key_str = f"{key[0]}_{key[1]}"
                else:
                    key_str = str(key)
                best_metrics_serializable[key_str] = value
        
        with open(BEST_CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump({
                'best_score': best_score,
                'best_params': best_params,
                'best_metrics': best_metrics_serializable,
            }, f, indent=2)


# ============================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================

def main():
    parser = argparse.ArgumentParser(description='ìë™ íŒŒë¼ë¯¸í„° íŠœë‹')
    parser.add_argument(
        '--trials',
        type=int,
        default=20,
        help='ì‹œí–‰ íšŸìˆ˜ (ê¸°ë³¸ê°’: 20, fast ëª¨ë“œ ê¶Œì¥)'
    )
    parser.add_argument(
        '--seed',
        type=int,
        default=None,
        help='ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: ìë™ ìƒì„±)'
    )
    parser.add_argument(
        '--profile',
        type=str,
        default='fast',
        choices=['fast', 'full'],
        help='í‰ê°€ í”„ë¡œíŒŒì¼: fast (ë¹ ë¥¸ íƒìƒ‰, ê¸°ë³¸ê°’) ë˜ëŠ” full (ìµœì¢… í‰ê°€)'
    )
    
    args = parser.parse_args()
    
    run_auto_tune(args.trials, args.seed, args.profile)


if __name__ == '__main__':
    main()

